Problem Statement
Design and implement a distributed online auction system running on 4 independent nodes (laptops) communicating via RPC in an asynchronous message-passing environment.
The system must:
â€¢	Ensure consistent agreement on highest bid
â€¢	Prevent concurrent conflicting updates
â€¢	Tolerate leader and participant failures
â€¢	Recover using coordinated checkpointing
â€¢	Detect transaction termination
â€¢	Use majority-based consensus for committing bids
â€¢	Maintain replica consistency across all nodes
The system simulates a simplified distributed transaction manager inspired by:
â€¢	Google Spanner
â€¢	Amazon Aurora
________________________________________
ğŸ— 2ï¸âƒ£ System Model
â€¢	Asynchronous distributed system
â€¢	Message-passing communication (Go RPC)
â€¢	Crash-stop failure model
â€¢	No shared memory
â€¢	Logical clock-based ordering
â€¢	Majority quorum (3/4)
________________________________________
ğŸ§  3ï¸âƒ£ Concepts Covered (Mapped to Syllabus)
________________________________________
ğŸ”¹ A. Models of Computation
âœ” Shared-nothing architecture
âœ” Message-passing system
âœ” Asynchronous system behavior
âœ” RPC-based communication
________________________________________
ğŸ”¹ B. Logical Time & Event Ordering
Implemented using:
â€¢	Lamport Logical Clocks
â€¢	Timestamped messages
â€¢	Total ordering of bids
Purpose:
â€¢	Resolve simultaneous bids
â€¢	Maintain consistent event ordering
________________________________________
ğŸ”¹ C. Distributed Mutual Exclusion
Algorithm used:
â€¢	Ricartâ€“Agrawala
Used to:
â€¢	Prevent simultaneous bid processing
â€¢	Ensure one transaction enters commit phase at a time
Demonstrates:
â€¢	Fairness
â€¢	No centralized locking
â€¢	Deadlock-free protocol
________________________________________
ğŸ”¹ D. Leader Election
Algorithm used:
â€¢	Bully Algorithm
Used to:
â€¢	Elect coordinator node
â€¢	Replace failed leader
â€¢	Maintain availability
Demonstrates:
â€¢	Failure detection
â€¢	Re-election
â€¢	Liveness guarantee
________________________________________
ğŸ”¹ E. Consensus & Agreement Problem
Consensus style:
â€¢	Majority quorum-based agreement
Implementation:
â€¢	Mini Two-Phase Commit
â€¢	Voting phase
â€¢	Commit/Abort decision
Demonstrates:
â€¢	Agreement problem
â€¢	Quorum logic
â€¢	Strong consistency model
________________________________________
ğŸ”¹ F. Commit Protocol
Two-Phase Commit (2PC):
Phase 1: PREPARE (Voting)
Phase 2: COMMIT or ABORT
Demonstrates:
â€¢	Atomicity
â€¢	Consistency
â€¢	Blocking problem of 2PC
________________________________________
ğŸ”¹ G. Fault Tolerance
Implemented using:
âœ” Crash-stop model
âœ” Leader re-election
âœ” Transaction logs
âœ” Recovery from checkpoint
âœ” Timeout detection
Failure Scenarios Handled:
â€¢	Leader crash before commit
â€¢	Participant crash during voting
â€¢	Network delay simulation
________________________________________
ğŸ”¹ H. Coordinated Checkpointing
Mechanism:
â€¢	Leader initiates global checkpoint
â€¢	All nodes save:
o	Highest bid
o	Bidder
o	Logical clock
o	Pending transactions
Checkpoint ensures:
â€¢	Consistent recovery state
â€¢	No orphan processes
â€¢	No inconsistent rollbacks
Demonstrates:
â€¢	Coordinated checkpoint protocol
â€¢	Recovery model
â€¢	Consistent global state
________________________________________
ğŸ”¹ I. Termination Detection
After commit:
â€¢	Leader waits for ACK from all participants
â€¢	Once all ACK received â†’ transaction considered globally terminated
Demonstrates:
â€¢	Distributed termination detection
â€¢	Completion guarantees
________________________________________
ğŸ”¹ J. Replica Management & Consistency
â€¢	All nodes maintain replicated auction state
â€¢	Commit only on majority agreement
â€¢	Strong consistency model
Consistency Type:
â€¢	Linearizable updates via quorum commit
________________________________________
ğŸ”¹ K. Concurrency Control
â€¢	Mutual exclusion ensures serializability
â€¢	Logical clock ordering ensures deterministic processing
Equivalent to:
â€¢	Strict serializable execution
________________________________________
ğŸ”¹ L. Fault Recovery
Upon restart:
1.	Load checkpoint
2.	Rejoin cluster
3.	Sync with leader
4.	Resume normal operation
Demonstrates:
â€¢	Recovery protocol
â€¢	State reconciliation
________________________________________
ğŸ”¹ M. Cloud & Distributed System Concepts
â€¢	Cluster-style architecture
â€¢	Coordinator-based transaction manager
â€¢	Similar to distributed DB transaction layer
Conceptually inspired by:
â€¢	Google Spanner
â€¢	Amazon Aurora

